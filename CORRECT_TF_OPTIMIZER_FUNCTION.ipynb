{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting at x: 2.0 y: 3.0 z: 15.0\n",
      "[(3.6, 1.8), (5.4, 2.7)]\n",
      "step 0 x: 1.8 y: 2.7 z: 12.530001\n",
      "\n",
      "[(3.24, 1.62), (4.86, 2.43)]\n",
      "step 1 x: 1.62 y: 2.43 z: 10.529301\n",
      "\n",
      "[(2.916, 1.458), (4.374, 2.187)]\n",
      "step 2 x: 1.458 y: 2.187 z: 8.908733\n",
      "\n",
      "[(2.6244, 1.3122), (3.9366, 1.9683)]\n",
      "step 3 x: 1.3122 y: 1.9683 z: 7.596073\n",
      "\n",
      "[(2.36196, 1.18098), (3.54294, 1.77147)]\n",
      "step 4 x: 1.18098 y: 1.77147 z: 6.5328197\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "\n",
    "max_iter = 5\n",
    "\n",
    "x = tf.Variable(2, name='x', dtype=tf.float32)\n",
    "y = tf.Variable(3, name='y', dtype=tf.float32)\n",
    "const = tf.constant(2.0, dtype=tf.float32)\n",
    "\n",
    "\n",
    "def function_to_minimize(x,y, const):\n",
    "    # z = x^2 + y^2 + constant\n",
    "    z = x**2 + y**2 + const\n",
    "    return z\n",
    "\n",
    "def calc_grad(x,y):\n",
    "    # z = 2x^2 + y^2 + constant\n",
    "    dz_dx = 2*x \n",
    "    dz_dy = 2*y \n",
    "    return [(dz_dx, x), (dz_dy, y)]\n",
    "\n",
    "\n",
    "z = function_to_minimize(x,y,const)\n",
    "\n",
    "\n",
    "#optimizer = tf.train.GradientDescentOptimizer(0.05)\n",
    "optimzer = tf.train.AdamOptimizer(learning_rate=0.1)\n",
    "\n",
    "init = tf.compat.v1.global_variables_initializer()\n",
    "\n",
    "def optimize():\n",
    "    with tf.Session() as session:\n",
    "        session.run(init)\n",
    "        print(\"starting at\", \"x:\", session.run(x), \"y:\", session.run(y), \"z:\", session.run(z))\n",
    "        for step in range(max_iter):\n",
    "            grads_and_vars = calc_grad(x,y)\n",
    "            train = optimizer.apply_gradients(grads_and_vars)\n",
    "            session.run(train)\n",
    "            yy = list(grads_and_vars)\n",
    "            ww = [(session.run(gradient), session.run(theta_Tensor)) for gradient, theta_Tensor in yy]\n",
    "            print(ww)\n",
    "            print(\"step\", step, \"x:\", session.run(x), \"y:\", session.run(y), \"z:\", session.run(z))\n",
    "            print('')\n",
    "\n",
    "optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Placeholder method!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOT WORKING WITH ADAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting at x: 2.0 y: 3.0 z: 15.0\n",
      "step 0 x: 1.8 y: 2.7 z: 12.530001\n",
      "step 1 x: 1.62 y: 2.43 z: 10.529301\n",
      "step 2 x: 1.458 y: 2.187 z: 8.908733\n",
      "step 3 x: 1.3122 y: 1.9683 z: 7.596073\n",
      "step 4 x: 1.18098 y: 1.77147 z: 6.5328197\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "\n",
    "max_iter = 5\n",
    "\n",
    "x = tf.Variable(2, name='x', dtype=tf.float32)\n",
    "y = tf.Variable(3, name='y', dtype=tf.float32)\n",
    "const = tf.constant(2.0, dtype=tf.float32)\n",
    "\n",
    "\n",
    "def function_to_minimize(x,y, const):\n",
    "    # z = x^2 + y^2 + constant\n",
    "    z = x**2 + y**2 + const\n",
    "    return z\n",
    "\n",
    "def calc_grad(x,y):\n",
    "    # z = 2x^2 + y^2 + constant\n",
    "    dz_dx = 2*x \n",
    "    dz_dy = 2*y \n",
    "    return [(dz_dx, x), (dz_dy, y)]\n",
    "\n",
    "\n",
    "z = function_to_minimize(x,y,const)\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.05)\n",
    "#optimizer = tf.train.AdamOptimizer(learning_rate=0.1)\n",
    "\n",
    "init = tf.compat.v1.global_variables_initializer()\n",
    "\n",
    "_, variables = zip(*optimizer.compute_gradients(z))\n",
    "\n",
    "\n",
    "dz_dx = tf.placeholder(tf.float32, shape=(), name='dz_dx')\n",
    "dz_dy = tf.placeholder(tf.float32, shape=(), name='dz_dy')\n",
    "grad_sub_dict = {'dz_dx': dz_dx, 'dz_dy': dz_dy}\n",
    "\n",
    "place_holder_list = [dz_dx, dz_dy]\n",
    "#= (*grad_place_holder_list, 0.0)  # extra 0.0 due to varibles having one extra value!\n",
    "\n",
    "train_step = optimizer.apply_gradients(zip(place_holder_list, variables))\n",
    "\n",
    "\n",
    "def optimize():\n",
    "    with tf.Session() as session:\n",
    "        session.run(init)\n",
    "        print(\"starting at\", \"x:\", session.run(x), \"y:\", session.run(y), \"z:\", session.run(z))\n",
    "        for step in range(max_iter):\n",
    "            grads_and_vars = calc_grad(session.run(x),session.run(y))\n",
    "            grad_sub_dict = {dz_dx: grads_and_vars[0][0], dz_dy: grads_and_vars[1][0]} # {placeholder: value}\n",
    "            #print(grad_sub_dict)\n",
    "            train_step.run(feed_dict=grad_sub_dict)\n",
    "            \n",
    "            print(\"step\", step, \"x:\", session.run(x), \"y:\", session.run(y), \"z:\", session.run(z))\n",
    "            \n",
    "\n",
    "optimize()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting at x: 2.0 y: 3.0 z: 15.0\n",
      "2.0\n",
      "step 0 x: 1.9 y: 2.9 z: 14.02\n",
      "2.0\n",
      "step 1 x: 1.9 y: 2.9 z: 14.02\n",
      "2.0\n",
      "step 2 x: 1.9 y: 2.9 z: 14.02\n",
      "2.0\n",
      "step 3 x: 1.9 y: 2.9 z: 14.02\n",
      "2.0\n",
      "step 4 x: 1.9 y: 2.9 z: 14.02\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "\n",
    "max_iter = 5\n",
    "\n",
    "x = tf.Variable(2, name='x', dtype=tf.float32)\n",
    "y = tf.Variable(3, name='y', dtype=tf.float32)\n",
    "const = tf.constant(2.0, dtype=tf.float32)\n",
    "\n",
    "\n",
    "def function_to_minimize(x,y, const):\n",
    "    # z = x^2 + y^2 + constant\n",
    "    z = x**2 + y**2 + const\n",
    "    return z\n",
    "\n",
    "def calc_grad(x,y):\n",
    "    # z = 2x^2 + y^2 + constant\n",
    "    dz_dx = 2*x \n",
    "    dz_dy = 2*y \n",
    "    return [(dz_dx, x), (dz_dy, y)]\n",
    "\n",
    "\n",
    "z = function_to_minimize(x,y,const)\n",
    "\n",
    "#optimizer = tf.train.GradientDescentOptimizer(0.05)\n",
    "\n",
    "# beta1 = tf.Variable(0.9)\n",
    "# beta2=tf.Variable(0.999)\n",
    "# optimizer = tf.train.AdamOptimizer(learning_rate=0.1, beta1= beta1, beta2=beta2)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.1, beta1= 0.9, beta2=0.999)\n",
    "\n",
    "init = tf.compat.v1.global_variables_initializer()\n",
    "\n",
    "_, variables = zip(*optimizer.compute_gradients(z))\n",
    "\n",
    "\n",
    "dz_dx = tf.placeholder(tf.float32, shape=(), name='dz_dx')\n",
    "dz_dy = tf.placeholder(tf.float32, shape=(), name='dz_dy')\n",
    "grad_sub_dict = {'dz_dx': dz_dx, 'dz_dy': dz_dy}\n",
    "\n",
    "place_holder_list = [dz_dx, dz_dy]\n",
    "#= (*grad_place_holder_list, 0.0)  # extra 0.0 due to varibles having one extra value!\n",
    "\n",
    "train_step = optimizer.apply_gradients(zip(place_holder_list, variables))\n",
    "\n",
    "\n",
    "def optimize():\n",
    "    with tf.Session() as session:\n",
    "        session.run(init)\n",
    "        print(\"starting at\", \"x:\", session.run(x), \"y:\", session.run(y), \"z:\", session.run(z))\n",
    "        for step in range(max_iter):\n",
    "            \n",
    "            #session.run(tf.compat.v1.initialize_variables([beta1, beta2]))\n",
    "            #optimizer = tf.train.AdamOptimizer(learning_rate=0.1, beta1= beta1, beta2=beta2)\n",
    "            session.run(tf.compat.v1.global_variables_initializer())\n",
    "            \n",
    "            print(session.run(x))\n",
    "            \n",
    "            grads_and_vars = calc_grad(session.run(x),session.run(y))\n",
    "            grad_sub_dict = {dz_dx: grads_and_vars[0][0], dz_dy: grads_and_vars[1][0]} # {placeholder: value}\n",
    "#             print(grad_sub_dict)\n",
    "#             print('')\n",
    "#             init2 = tf.compat.v1.global_variables_initializer()\n",
    "#             session.run(init2)\n",
    "            train_step.run(feed_dict=grad_sub_dict)\n",
    "\n",
    "            print(\"step\", step, \"x:\", session.run(x), \"y:\", session.run(y), \"z:\", session.run(z))\n",
    "            \n",
    "\n",
    "optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WORKING WITH ADAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting at x: 2.0 y: 3.0 z: 15.0\n",
      "step 0 x: 1.9 y: 2.9 z: 14.02\n",
      "##\n",
      "[(3.799999952316284, 1.9), (5.800000190734863, 2.9)]\n",
      "{<tf.Tensor 'dz_dx:0' shape=() dtype=float32>: 3.799999952316284, <tf.Tensor 'dz_dy:0' shape=() dtype=float32>: 5.800000190734863}\n",
      "##\n",
      "step 1 x: 1.8001668 y: 2.8001032 z: 13.081179\n",
      "##\n",
      "[(3.6003336906433105, 1.8001668), (5.60020637512207, 2.8001032)]\n",
      "{<tf.Tensor 'dz_dx:0' shape=() dtype=float32>: 3.6003336906433105, <tf.Tensor 'dz_dy:0' shape=() dtype=float32>: 5.60020637512207}\n",
      "##\n",
      "step 2 x: 1.7006245 y: 2.7003827 z: 12.18419\n",
      "##\n",
      "[(3.4012489318847656, 1.7006245), (5.400765419006348, 2.7003827)]\n",
      "{<tf.Tensor 'dz_dx:0' shape=() dtype=float32>: 3.4012489318847656, <tf.Tensor 'dz_dy:0' shape=() dtype=float32>: 5.400765419006348}\n",
      "##\n",
      "step 3 x: 1.6015062 y: 2.600915 z: 11.329581\n",
      "##\n",
      "[(3.203012466430664, 1.6015062), (5.20182991027832, 2.600915)]\n",
      "{<tf.Tensor 'dz_dx:0' shape=() dtype=float32>: 3.203012466430664, <tf.Tensor 'dz_dy:0' shape=() dtype=float32>: 5.20182991027832}\n",
      "##\n",
      "step 4 x: 1.5029573 y: 2.501781 z: 10.517789\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "\n",
    "max_iter = 5\n",
    "\n",
    "x = tf.Variable(2, name='x', dtype=tf.float32)\n",
    "y = tf.Variable(3, name='y', dtype=tf.float32)\n",
    "const = tf.constant(2.0, dtype=tf.float32)\n",
    "\n",
    "\n",
    "def function_to_minimize(x,y, const):\n",
    "    # z = x^2 + y^2 + constant\n",
    "    z = x**2 + y**2 + const\n",
    "    return z\n",
    "\n",
    "def calc_grad(x,y):\n",
    "    # z = 2x^2 + y^2 + constant\n",
    "    dz_dx = 2*x \n",
    "    dz_dy = 2*y \n",
    "    return [(dz_dx, x), (dz_dy, y)]\n",
    "\n",
    "\n",
    "z = function_to_minimize(x,y,const)\n",
    "\n",
    "#optimizer = tf.train.GradientDescentOptimizer(0.05)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.1, beta1= 0.9, beta2=0.999)\n",
    "\n",
    "init = tf.compat.v1.global_variables_initializer()\n",
    "\n",
    "_, variables = zip(*optimizer.compute_gradients(z))\n",
    "\n",
    "\n",
    "dz_dx = tf.placeholder(tf.float32, shape=(), name='dz_dx')\n",
    "dz_dy = tf.placeholder(tf.float32, shape=(), name='dz_dy')\n",
    "grad_sub_dict = {'dz_dx': dz_dx, 'dz_dy': dz_dy}\n",
    "\n",
    "place_holder_list = [dz_dx, dz_dy]\n",
    "#= (*grad_place_holder_list, 0.0)  # extra 0.0 due to varibles having one extra value!\n",
    "\n",
    "train_step = optimizer.apply_gradients(zip(place_holder_list, variables))\n",
    "\n",
    "\n",
    "def optimize():\n",
    "    with tf.Session() as session:\n",
    "        session.run(init)\n",
    "        print(\"starting at\", \"x:\", session.run(x), \"y:\", session.run(y), \"z:\", session.run(z))\n",
    "        for step in range(max_iter):\n",
    "            \n",
    "            if step == 0:\n",
    "                session.run(tf.compat.v1.global_variables_initializer())\n",
    "#                 print(session.run(x))\n",
    "                grads_and_vars = calc_grad(session.run(x),session.run(y))\n",
    "                grad_sub_dict = {dz_dx: grads_and_vars[0][0], dz_dy: grads_and_vars[1][0]} # {placeholder: value}\n",
    "                train_step.run(feed_dict=grad_sub_dict)\n",
    "                print(\"step\", step, \"x:\", session.run(x), \"y:\", session.run(y), \"z:\", session.run(z))\n",
    "            else:\n",
    "#                 print(session.run(x))\n",
    "                grads_and_vars = calc_grad(session.run(x),session.run(y))\n",
    "                grad_sub_dict = {dz_dx: grads_and_vars[0][0], dz_dy: grads_and_vars[1][0]} # {placeholder: value}\n",
    "                \n",
    "                print('##')\n",
    "                print(grads_and_vars)\n",
    "                print(grad_sub_dict)\n",
    "                print('##')\n",
    "                \n",
    "                train_step.run(feed_dict=grad_sub_dict)\n",
    "                print(\"step\", step, \"x:\", session.run(x), \"y:\", session.run(y), \"z:\", session.run(z))\n",
    "                \n",
    "            \n",
    "\n",
    "optimize()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
